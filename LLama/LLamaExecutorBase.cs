// Import interface definitions for LLama executors.
using LLama.Abstractions;                   // Provides interfaces (e.g., ILLamaExecutor) for executor implementations.
// Import common utility functions, types, and helpers.
using LLama.Common;                         // Contains general-purpose utilities and helper classes.
// Import custom exception types for LLama.
using LLama.Exceptions;                     // Defines exceptions specific to the LLama project.
// Import definitions and interop wrappers for native LLama functions.
using LLama.Native;                         // Enables interop with native LLama libraries.
// Import logging interfaces.
using Microsoft.Extensions.Logging;         // Provides logging capabilities.
// Import basic system definitions.
using System;                               // Provides core system types and functionalities.
// Import generic collections such as List<T>.
using System.Collections.Generic;           // Enables use of generic collections (e.g., List<T>).
// Import types for file input/output.
using System.IO;                            // Supports file reading and writing operations.
// Import LINQ query capabilities.
using System.Linq;                          // Provides LINQ methods for collection manipulation.
// Import compiler services attributes.
using System.Runtime.CompilerServices;      // Supports attributes like EnumeratorCancellation.
// Import JSON serialization attributes.
using System.Text.Json.Serialization;       // Allows customizing JSON serialization via attributes.
// Import threading types (e.g., CancellationToken).
using System.Threading;                     // Provides threading primitives and cancellation support.
// Import asynchronous programming types (e.g., Task, async/await).
using System.Threading.Tasks;               // Supports asynchronous programming constructs.

namespace LLama  // Define the LLama namespace.
{
    /// <summary>
    /// The base class for stateful LLama executors.
    /// This class provides core functionality for token management, session handling,
    /// context recycling, and asynchronous inference execution.
    /// </summary>
    public abstract class StatefulExecutorBase : ILLamaExecutor  // Implements the ILLamaExecutor interface.
    {
        // The logger instance used for diagnostic output.
        protected ILogger? _logger;  // Nullable logger for logging messages.

        // The count of tokens that have been processed before the current inference (n_past).
        protected int _pastTokensCount;  // Tracks the total number of tokens processed in past inferences.
        // The count of tokens that have been consumed during the current inference (n_consume).
        protected int _consumedTokensCount;  // Tracks the number of input tokens already consumed.

        // The number of tokens from the session file that have been consumed.
        protected int _n_session_consumed;  // Tracks how many tokens from a loaded session have been used.
        // The count of tokens matching between the session file and the current prompt.
        protected int _n_matching_session_tokens;  // Indicates how many tokens in the session match the current prompt.

        // The file path of the session file used for storing and restoring session state.
        protected string? _pathSession;  // Holds the session file path (if any).

        // A container for tokens that have been produced (or are queued to be processed).
        protected List<LLamaToken> _embeds = new();  // List holding tokens generated by the model.
        // A container for tokens obtained from the input prompt.
        protected List<LLamaToken> _embed_inps = new();  // List holding tokenized input prompt tokens.
        // A container for tokens loaded from a session file.
        protected List<LLamaToken> _session_tokens = new();  // List holding tokens retrieved from a session file.

        // A fixed-size queue to hold the most recent tokens generated by the model.
        protected FixedSizeQueue<LLamaToken> _last_n_tokens;  // Queue for storing a fixed number of latest tokens.

        // The LLama context used for tokenization, inference, and accessing model parameters.
        public LLamaContext Context { get; }  // Read-only property exposing the LLama context.

        // -- Multimodal support (LLava) --

        /// <inheritdoc />
        public bool IsMultiModal  // Indicates whether multimodal functionality is enabled.
        {
            get
            {
                // Return true if a CLIP/LLava model is provided.
                return ClipModel != null;
            }
        }

        /// <inheritdoc />
        public LLavaWeights? ClipModel { get; }  // Optional clip model for image embeddings; null if not in multimodal mode.

        /// <summary>
        /// A container for image data, where each image is represented as a byte array.
        /// In multimodal mode, these images are embedded into the model's processing.
        /// </summary>
        public List<byte[]> Images { get; }  // List that holds images as byte arrays.

        // A streaming decoder that converts sequences of tokens into output strings.
        private readonly StreamingTokenDecoder _decoder;  // Decoder to transform tokens into human-readable text.

        /// <summary>
        /// Primary constructor for the stateful executor.
        /// Initializes core fields, token counters, session storage, and sets up the token history and streaming decoder.
        /// </summary>
        /// <param name="context">The LLama context used for inference and tokenization.</param>
        /// <param name="logger">Optional logger for diagnostic output.</param>
        protected StatefulExecutorBase(LLamaContext context, ILogger? logger = null)
        {
            // Initialize the list to hold image data (for multimodal operation).
            Images = new List<byte[]>();
            // Store the provided logger instance.
            _logger = logger;
            // Store the LLama context for later use.
            Context = context;
            // Initialize token counters.
            _pastTokensCount = 0;          // No tokens processed initially.
            _consumedTokensCount = 0;      // No tokens consumed at start.
            _n_session_consumed = 0;       // No session tokens consumed.
            // Create a fixed-size queue for the last tokens with capacity set to the model's context size.
            _last_n_tokens = new FixedSizeQueue<LLamaToken>((int)Context.ContextSize);
            // Initialize the streaming token decoder with the current context.
            _decoder = new StreamingTokenDecoder(context);
        }
        
        /// <summary>
        /// Overloaded constructor that accepts LLavaWeights for multimodal operation.
        /// Calls the primary constructor and then sets the clip model for image embeddings.
        /// </summary>
        /// <param name="context">The LLama context for inference and tokenization.</param>
        /// <param name="lLavaWeights">The LLavaWeights instance used for image embeddings.</param>
        /// <param name="logger">Optional logger for diagnostic output.</param>
        public StatefulExecutorBase(LLamaContext context, LLavaWeights lLavaWeights, ILogger? logger = null)
            : this(context, logger)  // Call the primary constructor.
        {
            // Set the clip model to enable multimodal support.
            ClipModel = lLavaWeights;
        }

        /// <summary>
        /// Configures the executor to use a session file.
        /// The session file contains precomputed tokens (KV cache) for faster prompt initialization.
        /// </summary>
        /// <param name="filename">The path to the session file.</param>
        /// <returns>The current executor instance (for chaining).</returns>
        /// <exception cref="ArgumentNullException">Thrown if filename is null or empty.</exception>
        /// <exception cref="RuntimeError">Thrown if the session file fails to load.</exception>
        public StatefulExecutorBase WithSessionFile(string filename)
        {
            // Store the session file path.
            _pathSession = filename;
            // Ensure the filename is not null or empty.
            if (string.IsNullOrEmpty(filename))
            {
                throw new ArgumentNullException(nameof(filename), "File name cannot be empty.");
            }
            // Check if the session file exists on disk.
            if (File.Exists(filename))
            {
                // Log that we are attempting to load the session.
                _logger?.LogInformation($"[LLamaExecutor] Attempting to load saved session from {filename}");
                // Allocate an array to hold tokens up to the model's context size.
                var session_tokens = new LLamaToken[Context.ContextSize];
                // Attempt to load the session tokens using the native API.
                if (!NativeApi.llama_state_load_file(Context.NativeHandle, _pathSession, session_tokens, (ulong)Context.ContextSize, out var n_token_count_out))
                {
                    // Log an error if the session file could not be loaded.
                    _logger?.LogError($"[LLamaExecutor] Failed to load session file {filename}");
                    // Throw an exception indicating the session load failure.
                    throw new RuntimeError($"Failed to load session file {_pathSession}");
                }
                // Retain only the tokens that were successfully loaded.
                _session_tokens = session_tokens.Take((int)n_token_count_out).ToList();
                // Log the number of tokens loaded from the session.
                _logger?.LogInformation($"[LLamaExecutor] Loaded a session with prompt size of {session_tokens.Length} tokens");
            }
            else
            {
                // Log a warning if the session file does not exist.
                _logger?.LogWarning("[LLamaExecutor] Session file does not exist, will create");
            }

            // Initialize the count of matching tokens between the session and the current prompt.
            _n_matching_session_tokens = 0;
            // If session tokens exist, attempt to match them with the current input tokens.
            if (_session_tokens.Count > 0)
            {
                foreach (var id in _session_tokens)
                {
                    // Break if we've processed all current input tokens or if a token doesn't match.
                    if (_n_matching_session_tokens >= _embed_inps.Count || id != _embed_inps[_n_matching_session_tokens])
                    {
                        break;
                    }
                    // Increase the matching token count.
                    _n_matching_session_tokens++;
                }
                // Log based on the degree of match between session tokens and the prompt.
                if (_n_matching_session_tokens >= _embed_inps.Count)
                {
                    _logger?.LogInformation("[LLamaExecutor] Session file has exact match for prompt!");
                }
                else if (_n_matching_session_tokens < _embed_inps.Count / 2)
                {
                    _logger?.LogWarning($"[LLamaExecutor] Session file has low similarity to prompt ({_n_matching_session_tokens} / {_embed_inps.Count} tokens) will mostly be reevaluated");
                }
                else
                {
                    _logger?.LogInformation($"[LLamaExecutor] Session file matches {_n_matching_session_tokens} / {_embed_inps.Count} tokens of prompt");
                }
            }

            // Return this instance to allow method chaining.
            return this;
        }

        /// <summary>
        /// Saves the current session state to a file.
        /// This writes the tokens from the session (KV cache) to disk for faster prompt reloading.
        /// </summary>
        /// <param name="filename">The path to the file where the session should be saved.</param>
        public void SaveSessionFile(string filename)
        {
            // Convert the session tokens list to an array.
            var session_token_array = _session_tokens.ToArray();
            // Use the native API to save the session state (KV cache) to the specified file.
            NativeApi.llama_state_save_file(Context.NativeHandle, filename, session_token_array, (ulong)session_token_array.Length);
        }

        /// <summary>
        /// Handles the case when the model runs out of context.
        /// It retains a portion of the original prompt tokens and recomputes logits in batches.
        /// </summary>
        /// <param name="tokensToKeep">The number of prompt tokens to retain.</param>
        protected virtual void HandleRunOutOfContext(int tokensToKeep)
        {
            // Calculate the number of tokens beyond the tokens to keep that have been processed.
            int n_left = _pastTokensCount - tokensToKeep;
            // Determine the number of tokens to discard (using half of the remaining tokens).
            int n_discard = n_left / 2;

            // Remove the key-value cache entries for tokens in the range [tokensToKeep, tokensToKeep + n_discard).
            NativeApi.llama_kv_cache_seq_rm(Context.NativeHandle, LLamaSeqId.Zero, tokensToKeep, tokensToKeep + n_discard);
            // Adjust the indices of the remaining cache entries to account for the discarded tokens.
            NativeApi.llama_kv_cache_seq_add(Context.NativeHandle, LLamaSeqId.Zero, tokensToKeep + n_discard, _pastTokensCount, -n_discard);

            // Update the past tokens count by subtracting the number of discarded tokens.
            _pastTokensCount -= n_discard;
            // Clear the session file path once context is reduced (stop saving session state).
            _pathSession = string.Empty;
        }

        /// <summary>
        /// Attempts to reuse a matching prefix from the session file.
        /// Reusing previously computed logits reduces computation if parts of the prompt match the session.
        /// </summary>
        protected virtual void TryReuseMatchingPrefix()
        {
            // Only proceed if there are session tokens that haven't been consumed.
            if (_n_session_consumed < _session_tokens.Count)
            {
                int i = 0;  // Initialize an index to track matching tokens.
                // Iterate over tokens in the _embeds list.
                for (; i < _embeds.Count; i++)
                {
                    // If a token does not match the corresponding session token...
                    if (_embeds[i] != _session_tokens[_n_session_consumed])
                    {
                        // Truncate the session tokens list to only the matched portion.
                        _session_tokens = _session_tokens.Take(_n_session_consumed).ToList();
                        break;
                    }
                    // Increase both the count of past tokens and the session tokens consumed.
                    _pastTokensCount++;
                    _n_session_consumed++;

                    // If all session tokens have been consumed, adjust the index and exit the loop.
                    if (_n_session_consumed >= _session_tokens.Count)
                    {
                        i++;
                        break;
                    }
                }

                // Remove the matching tokens from the _embeds list.
                if (i > 0)
                {
                    _embeds.RemoveRange(0, i);
                }
            }
        }

        /// <summary>
        /// Abstract method to decide whether to continue the generation loop.
        /// This is implemented by derived classes based on mode (e.g., instruct or interactive).
        /// </summary>
        /// <param name="args">Inference state arguments (e.g., remaining tokens).</param>
        /// <returns>A Task that resolves to a boolean indicating whether to continue generation.</returns>
        protected abstract Task<bool> GetLoopCondition(InferStateArgs args);

        /// <summary>
        /// Abstract method to preprocess the inputs before inference.
        /// Derived classes implement this to tokenize and prepare the prompt or continuation.
        /// </summary>
        /// <param name="text">The input prompt (or null for continuation).</param>
        /// <param name="args">Inference state arguments to be updated.</param>
        protected abstract Task PreprocessInputs(string? text, InferStateArgs args);

        /// <summary>
        /// Abstract method for post-processing after inference.
        /// Derived classes implement this to decide whether generation should stop or if extra output is needed.
        /// </summary>
        /// <param name="inferenceParams">Parameters controlling sampling behavior.</param>
        /// <param name="args">Inference state arguments.</param>
        /// <returns>
        /// A Task that resolves to a tuple:
        ///   - bool: indicating whether to break generation.
        ///   - IReadOnlyList&lt;string&gt;: any extra output strings to yield.
        /// </returns>
        protected abstract Task<(bool, IReadOnlyList<string>)> PostProcess(IInferenceParams inferenceParams, InferStateArgs args);

        /// <summary>
        /// Abstract method for the core inference logic.
        /// Responsible for generating tokens and updating the internal state.
        /// </summary>
        /// <param name="inferenceParams">Parameters controlling sampling and decoding.</param>
        /// <param name="args">Inference state arguments.</param>
        protected abstract Task InferInternal(IInferenceParams inferenceParams, InferStateArgs args);

        /// <summary>
        /// Abstract method to save the current state to a file.
        /// Derived classes implement this to serialize and write state data.
        /// </summary>
        /// <param name="filename">The file path to save the state.</param>
        public abstract Task SaveState(string filename);

        /// <summary>
        /// Abstract method to retrieve the current state data.
        /// Derived classes implement this to return an object representing the state.
        /// </summary>
        /// <returns>An object containing the current state.</returns>
        public abstract ExecutorBaseState GetStateData();

        /// <summary>
        /// Abstract method to load state from the provided state data.
        /// Derived classes implement this to restore internal state.
        /// </summary>
        /// <param name="data">State data to load.</param>
        public abstract Task LoadState(ExecutorBaseState data);

        /// <summary>
        /// Abstract method to load state from a file.
        /// Derived classes implement this to deserialize state data from disk.
        /// </summary>
        /// <param name="filename">The file path containing the state.</param>
        public abstract Task LoadState(string filename);

        /// <summary>
        /// Executes the inference asynchronously.
        /// This method yields output tokens (as strings) as they are generated.
        /// </summary>
        /// <param name="text">
        /// The prompt to begin inference. If null, generation continues from the current state.
        /// </param>
        /// <param name="inferenceParams">
        /// Parameters controlling inference (if null, defaults are used).
        /// </param>
        /// <param name="cancellationToken">
        /// A token to observe for cancellation requests.
        /// </param>
        /// <returns>An asynchronous stream of generated output strings.</returns>
        public virtual async IAsyncEnumerable<string> InferAsync(
            string? text,
            IInferenceParams? inferenceParams = null,
            [EnumeratorCancellation] CancellationToken cancellationToken = default)
        {
            // Check for immediate cancellation.
            cancellationToken.ThrowIfCancellationRequested();
            // Use provided inference parameters or create default ones.
            inferenceParams ??= new InferenceParams();

            // Initialize the state arguments for this inference run.
            var args = new InferStateArgs
            {
                // Copy the anti-prompts from inference parameters.
                Antiprompts = new List<string>(inferenceParams.AntiPrompts),
                // Set the token budget.
                RemainedTokens = inferenceParams.MaxTokens,
                ReturnValue = false,
                WaitForInput = false,
                // Determine whether to save the session based on session file and matching tokens.
                NeedToSaveSession = !string.IsNullOrEmpty(_pathSession) && _n_matching_session_tokens < _embed_inps.Count
            };

            // Preprocess the input prompt (or continuation).
            await PreprocessInputs(text, args);

            // Main generation loop.
            while (await GetLoopCondition(args))
            {
                // Check if cancellation has been requested.
                if (cancellationToken.IsCancellationRequested)
                {
                    break;
                }
                // Perform the core inference, which decodes and samples tokens.
                await InferInternal(inferenceParams, args);

                // If new tokens have been generated (flag is set)...
                if (args.ReturnValue)
                {
                    // Add the newly generated tokens to the streaming decoder.
                    _decoder.AddRange(_embeds);
                    // Yield the current decoded string.
                    yield return _decoder.Read();
                }

                // Post-process to decide whether generation should stop or extra outputs should be emitted.
                var (breakGeneration, extraOutputs) = await PostProcess(inferenceParams, args);
                if (extraOutputs is { Count: > 0 })
                {
                    // Yield any extra output strings.
                    foreach (var item in extraOutputs)
                    {
                        yield return item;
                    }
                }
                // Break out of the loop if instructed.
                if (breakGeneration)
                {
                    break;
                }
            }
        }

        /// <summary>
        /// Asynchronously pre-fills the prompt through the model to compute the KV cache,
        /// without generating any new tokens. This can reduce latency when the user eventually inputs text.
        /// </summary>
        /// <param name="prompt">The prompt to prefill.</param>
        /// <returns>A Task representing the asynchronous prefill operation.</returns>
        public virtual async Task PrefillPromptAsync(string prompt)
        {
            // Create inference parameters with zero tokens to generate.
            var inferenceParams = new InferenceParams
            {
                MaxTokens = 0
            };
            // Set up state arguments for pre-filling; no tokens will be generated and we wait for input.
            var args = new InferStateArgs
            {
                Antiprompts = new List<string>(),
                RemainedTokens = 0,
                ReturnValue = false,
                WaitForInput = true,
                NeedToSaveSession = false
            };

            // Preprocess the prompt.
            await PreprocessInputs(prompt, args);
            // First inference pass: adds the prompt to the embeds.
            await InferInternal(inferenceParams, args);
            // Second inference pass: processes the prompt through decode (updates KV cache).
            await InferInternal(inferenceParams, args);
        }

        /// <summary>
        /// Internal class holding state arguments used during a single inference run.
        /// </summary>
        protected class InferStateArgs
        {
            /// <summary>
            /// A list of antiprompt strings used to determine when to stop or wait for input.
            /// </summary>
            public IList<string>? Antiprompts { get; set; }
            /// <summary>
            /// The remaining number of tokens allowed to be generated.
            /// </summary>
            public int RemainedTokens { get; set; }
            /// <summary>
            /// Flag indicating if a new token was generated and should be returned.
            /// </summary>
            public bool ReturnValue { get; set; }
            /// <summary>
            /// Flag indicating if the executor should wait for additional input.
            /// </summary>
            public bool WaitForInput { get; set; }
            /// <summary>
            /// Flag indicating whether the session should be saved after inference.
            /// </summary>
            public bool NeedToSaveSession { get; set; }
        }

        /// <summary>
        /// Base state data for an executor.
        /// This class is used for saving and restoring the state of an executor.
        /// </summary>
        [JsonConverter(typeof(PolymorphicJSONConverter<ExecutorBaseState>))]
        public class ExecutorBaseState
        {
            // The count of tokens processed before the current inference.
            [JsonPropertyName("n_past")]
            public int PastTokensCount { get; set; }

            // The count of tokens consumed during the current inference.
            [JsonPropertyName("n_consumed")]
            public int ConsumedTokensCount { get; set; }

            // The count of session tokens that have been consumed.
            [JsonPropertyName("n_session_consumed")]
            public int ConsumedSessionCount { get; set; }

            // The number of tokens in the session that match the prompt.
            [JsonPropertyName("n_matching_session_tokens")]
            public int MatchingSessionTokensCount { get; set; }

            // The file path of the session file.
            [JsonPropertyName("path_session")]
            public string? SessionFilePath { get; set; }

            // The tokens that were generated/processed.
            [JsonPropertyName("embd")]
            public LLamaToken[] Embeds { get; set; }

            // The tokens obtained from the input prompt.
            [JsonPropertyName("embd_inps")]
            public LLamaToken[] EmbedInps { get; set; }

            // The tokens loaded from a session file.
            [JsonPropertyName("session_tokens")]
            public LLamaToken[] SessionTokens { get; set; }

            // The last tokens generated by the model.
            [JsonPropertyName("last_n_tokens")]
            public LLamaToken[] LastTokens { get; set; }

            // The maximum capacity of the last tokens queue.
            [JsonPropertyName("last_tokens_maximum_count")]
            public int LastTokensCapacity { get; set; }

            // Optionally, the Mirostat mu parameter if using Mirostat sampling.
            [JsonPropertyName("mirostat_mu")]
            public float? MirostatMu { get; set; }
        }
    }
}
