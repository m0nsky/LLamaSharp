// Import necessary namespaces from LLama libraries and .NET standard libraries.
using LLama.Abstractions;                   // Provides interface definitions for LLama executors.
using LLama.Common;                         // Contains common utilities, types, and helpers.
using LLama.Exceptions;                     // Contains custom exception types for LLama.
using LLama.Native;                         // Contains definitions and interop wrappers for native LLama functions.
using Microsoft.Extensions.Logging;         // Provides logging interfaces.
using System;                               // Provides basic system definitions.
using System.Collections.Generic;           // Provides generic collections (e.g., List<T>).
using System.IO;                            // Provides types for file input/output.
using System.Linq;                          // Provides LINQ query capabilities.
using System.Runtime.CompilerServices;      // Provides attributes for compiler services (e.g., EnumeratorCancellation).
using System.Text.Json.Serialization;       // Provides attributes for JSON serialization.
using System.Threading;                     // Provides types for threading and cancellation.
using System.Threading.Tasks;               // Provides types for asynchronous programming.

namespace LLama
{
    /// <summary>
    /// The base class for stateful LLama executors.
    /// This class provides core functionality for token management, session handling,
    /// context recycling, and asynchronous inference execution.
    /// </summary>
    public abstract class StatefulExecutorBase : ILLamaExecutor
    {
        // The logger used by this executor.
        protected ILogger? _logger;

        // The count of tokens that have already been processed by the model (n_past).
        protected int _pastTokensCount;
        // The count of tokens that have been consumed during the current inference (n_consume).
        protected int _consumedTokensCount;

        // Number of tokens from the session file that have already been consumed.
        protected int _n_session_consumed;
        // Number of tokens matching between the session file and current prompt.
        protected int _n_matching_session_tokens;

        // The path to the session file used for storing/restoring session state.
        protected string? _pathSession;

        // A container for the tokens that have been produced (or are about to be processed).
        protected List<LLamaToken> _embeds = new(); // Represents "embd" tokens.
        // A container for the tokens obtained from the input prompt.
        protected List<LLamaToken> _embed_inps = new();
        // A container for tokens stored from a session file.
        protected List<LLamaToken> _session_tokens = new();

        // A fixed-size queue holding the most recent tokens generated by the model.
        protected FixedSizeQueue<LLamaToken> _last_n_tokens;

        // The LLama context, used for tokenization, inference, and access to model parameters.
        public LLamaContext Context { get; }

        // LLava Section for multimodal support:
        //
        /// <inheritdoc />
        public bool IsMultiModal
        {
            get
            {
                // If a CLIP/LLava model is provided, we are operating in multimodal mode.
                return ClipModel != null;
            }
        }

        /// <inheritdoc />
        public LLavaWeights? ClipModel { get; }

        /// <summary>
        /// A container for image data (each image represented as a byte array).
        /// In multimodal mode, these images are embedded into the model's processing.
        /// </summary>
        public List<byte[]> Images { get; }

        // A streaming decoder that converts sequences of tokens into strings (for output).
        private readonly StreamingTokenDecoder _decoder;

        /// <summary>
        /// Primary constructor for the stateful executor.
        /// Initializes core fields and sets up the token history and streaming decoder.
        /// </summary>
        /// <param name="context">The LLama context used for inference and tokenization.</param>
        /// <param name="logger">Optional logger for diagnostic output.</param>
        protected StatefulExecutorBase(LLamaContext context, ILogger? logger = null)
        {
            // Initialize the list that will hold image data.
            Images = new List<byte[]>();
            // Store the provided logger.
            _logger = logger;
            // Store the LLama context.
            Context = context;
            // Initialize token counters.
            _pastTokensCount = 0;
            _consumedTokensCount = 0;
            _n_session_consumed = 0;
            // Create a fixed-size queue for the last tokens with capacity equal to the model's context size.
            _last_n_tokens = new FixedSizeQueue<LLamaToken>((int)Context.ContextSize);
            // Initialize the streaming decoder with the current context.
            _decoder = new StreamingTokenDecoder(context);
        }
        
        /// <summary>
        /// Overloaded constructor that accepts LLavaWeights for multimodal operation.
        /// Calls the primary constructor and then sets the multimodal clip model.
        /// </summary>
        /// <param name="context">The LLama context used for inference and tokenization.</param>
        /// <param name="lLavaWeights">The LLavaWeights instance used for image embeddings.</param>
        /// <param name="logger">Optional logger for diagnostic output.</param>
        public StatefulExecutorBase(LLamaContext context, LLavaWeights lLavaWeights, ILogger? logger = null)
            : this(context, logger)
        {
            // Set the clip model for multimodal support.
            ClipModel = lLavaWeights;
        }

        /// <summary>
        /// Configures the executor to use a session file.
        /// This file contains precomputed tokens (KV cache) for faster prompt initialization.
        /// </summary>
        /// <param name="filename">The path to the session file.</param>
        /// <returns>The current executor instance (for chaining).</returns>
        /// <exception cref="ArgumentNullException">Thrown if filename is null or empty.</exception>
        /// <exception cref="RuntimeError">Thrown if the session file fails to load.</exception>
        public StatefulExecutorBase WithSessionFile(string filename)
        {
            // Store the session file path.
            _pathSession = filename;
            // Ensure the filename is not empty.
            if (string.IsNullOrEmpty(filename))
            {
                throw new ArgumentNullException(nameof(filename), "File name cannot be empty.");
            }
            // If the file exists, attempt to load the session tokens.
            if (File.Exists(filename))
            {
                _logger?.LogInformation($"[LLamaExecutor] Attempting to load saved session from {filename}");
                // Allocate an array with size equal to the model's context size.
                var session_tokens = new LLamaToken[Context.ContextSize];
                // Attempt to load the session state from file.
                if (!NativeApi.llama_state_load_file(Context.NativeHandle, _pathSession, session_tokens, (ulong)Context.ContextSize, out var n_token_count_out))
                {
                    _logger?.LogError($"[LLamaExecutor] Failed to load session file {filename}");
                    throw new RuntimeError($"Failed to load session file {_pathSession}");
                }
                // Retain only the tokens that were loaded successfully.
                _session_tokens = session_tokens.Take((int)n_token_count_out).ToList();
                _logger?.LogInformation($"[LLamaExecutor] Loaded a session with prompt size of {session_tokens.Length} tokens");
            }
            else
            {
                _logger?.LogWarning("[LLamaExecutor] Session file does not exist, will create");
            }

            // Initialize the count of matching tokens between session and prompt.
            _n_matching_session_tokens = 0;
            // If there are any session tokens loaded, try to match them with the current input tokens.
            if (_session_tokens.Count > 0)
            {
                foreach (var id in _session_tokens)
                {
                    // If we've exhausted the current input tokens or the tokens do not match, break out.
                    if (_n_matching_session_tokens >= _embed_inps.Count || id != _embed_inps[_n_matching_session_tokens])
                    {
                        break;
                    }
                    // Increase the count of matching session tokens.
                    _n_matching_session_tokens++;
                }
                // Log details based on how many tokens match the current prompt.
                if (_n_matching_session_tokens >= _embed_inps.Count)
                {
                    _logger?.LogInformation("[LLamaExecutor] Session file has exact match for prompt!");
                }
                else if (_n_matching_session_tokens < _embed_inps.Count / 2)
                {
                    _logger?.LogWarning($"[LLamaExecutor] Session file has low similarity to prompt ({_n_matching_session_tokens} / {_embed_inps.Count} tokens) will mostly be reevaluated");
                }
                else
                {
                    _logger?.LogInformation($"[LLamaExecutor] Session file matches {_n_matching_session_tokens} / {_embed_inps.Count} tokens of prompt");
                }
            }

            // Return the executor instance for method chaining.
            return this;
        }

        /// <summary>
        /// Saves the current session state to a file.
        /// This writes the tokens from the session (KV cache) to disk for faster prompt reloading.
        /// </summary>
        /// <param name="filename">The path to the file where the session should be saved.</param>
        public void SaveSessionFile(string filename)
        {
            // Convert the session tokens to an array.
            var session_token_array = _session_tokens.ToArray();
            // Use the native API to save the session state to file.
            NativeApi.llama_state_save_file(Context.NativeHandle, filename, session_token_array, (ulong)session_token_array.Length);
        }

        /// <summary>
        /// Handles the case when the model runs out of context.
        /// It retains some tokens from the original prompt and recomputes logits in batches.
        /// </summary>
        /// <param name="tokensToKeep">The number of tokens from the prompt to keep.</param>
        protected virtual void HandleRunOutOfContext(int tokensToKeep)
        {
            // Calculate how many tokens beyond the ones to keep have been processed.
            int n_left = _pastTokensCount - tokensToKeep;
            // Determine how many tokens to discard (half of the remaining tokens).
            int n_discard = n_left / 2;

            // Remove the key-value cache entries for tokens in the range [tokensToKeep, tokensToKeep + n_discard).
            NativeApi.llama_kv_cache_seq_rm(Context.NativeHandle, LLamaSeqId.Zero, tokensToKeep, tokensToKeep + n_discard);
            // Re-index the remaining cache entries to account for the discarded tokens.
            NativeApi.llama_kv_cache_seq_add(Context.NativeHandle, LLamaSeqId.Zero, tokensToKeep + n_discard, _pastTokensCount, -n_discard);

            // Update the count of past tokens.
            _pastTokensCount -= n_discard;
            // Once context is reduced, stop saving the session.
            _pathSession = string.Empty;
        }

        /// <summary>
        /// Attempts to reuse a matching prefix from the session file.
        /// This reduces computation by reusing previously computed logits.
        /// </summary>
        protected virtual void TryReuseMatchingPrefix()
        {
            // If there are still session tokens that haven't been consumed...
            if (_n_session_consumed < _session_tokens.Count)
            {
                int i = 0;
                // Iterate through the current embeds.
                for (; i < _embeds.Count; i++)
                {
                    // If a token does not match the corresponding session token...
                    if (_embeds[i] != _session_tokens[_n_session_consumed])
                    {
                        // Truncate the session tokens to only the matching part.
                        _session_tokens = _session_tokens.Take(_n_session_consumed).ToList();
                        break;
                    }
                    // Increase the count of past tokens and session tokens consumed.
                    _pastTokensCount++;
                    _n_session_consumed++;

                    // If we've consumed all session tokens, adjust the index and break.
                    if (_n_session_consumed >= _session_tokens.Count)
                    {
                        i++;
                        break;
                    }
                }

                // Remove the tokens that were successfully matched from the embeds list.
                if (i > 0)
                {
                    _embeds.RemoveRange(0, i);
                }
            }
        }

        /// <summary>
        /// Decide whether to continue the generation loop.
        /// This is implemented by derived classes.
        /// </summary>
        /// <param name="args">Inference state arguments (e.g., remaining tokens).</param>
        /// <returns>A task that resolves to a boolean indicating whether to continue.</returns>
        protected abstract Task<bool> GetLoopCondition(InferStateArgs args);

        /// <summary>
        /// Preprocess the inputs before inference.
        /// This method is implemented by derived classes (e.g., interactive, instruct modes).
        /// </summary>
        /// <param name="text">The input prompt (or null for continuation).</param>
        /// <param name="args">Inference state arguments to update.</param>
        protected abstract Task PreprocessInputs(string? text, InferStateArgs args);

        /// <summary>
        /// Do post processing after inference.
        /// For example, decide whether generation should stop or if additional output is needed.
        /// </summary>
        /// <param name="inferenceParams">The inference parameters.</param>
        /// <param name="args">Inference state arguments.</param>
        /// <returns>
        /// A task that resolves to a tuple:
        ///   - bool: whether to break generation.
        ///   - IReadOnlyList&lt;string&gt;: any extra outputs to yield.
        /// </returns>
        protected abstract Task<(bool, IReadOnlyList<string>)> PostProcess(IInferenceParams inferenceParams, InferStateArgs args);

        /// <summary>
        /// The core inference logic.
        /// This method is responsible for generating tokens and updating state.
        /// </summary>
        /// <param name="inferenceParams">The parameters controlling inference (e.g., sampling settings).</param>
        /// <param name="args">Inference state arguments.</param>
        protected abstract Task InferInternal(IInferenceParams inferenceParams, InferStateArgs args);

        /// <summary>
        /// Save the current state to a file.
        /// Implemented by derived classes.
        /// </summary>
        /// <param name="filename">The path to the file where state should be saved.</param>
        public abstract Task SaveState(string filename);

        /// <summary>
        /// Get the current state data.
        /// Implemented by derived classes.
        /// </summary>
        /// <returns>An object representing the current state.</returns>
        public abstract ExecutorBaseState GetStateData();

        /// <summary>
        /// Load the state from the provided data.
        /// Implemented by derived classes.
        /// </summary>
        /// <param name="data">State data to load.</param>
        public abstract Task LoadState(ExecutorBaseState data);

        /// <summary>
        /// Load the state from a file.
        /// Implemented by derived classes.
        /// </summary>
        /// <param name="filename">The path to the file containing the state.</param>
        public abstract Task LoadState(string filename);

        /// <summary>
        /// Execute the inference asynchronously.
        /// This method yields output tokens (as strings) as they are generated.
        /// </summary>
        /// <param name="text">
        /// The prompt to begin inference. If null, generation continues from the current state.
        /// </param>
        /// <param name="inferenceParams">
        /// Parameters controlling inference (if not provided, defaults are used).
        /// </param>
        /// <param name="cancellationToken">
        /// A token to monitor for cancellation requests.
        /// </param>
        /// <returns>An asynchronous stream of generated output strings.</returns>
        public virtual async IAsyncEnumerable<string> InferAsync(
            string? text,
            IInferenceParams? inferenceParams = null,
            [EnumeratorCancellation] CancellationToken cancellationToken = default)
        {
            // Check if cancellation was already requested.
            cancellationToken.ThrowIfCancellationRequested();
            // Use provided inference parameters or create defaults.
            inferenceParams ??= new InferenceParams();

            // Initialize the state arguments for this inference run.
            var args = new InferStateArgs
            {
                // Copy the anti-prompts from the inference parameters.
                Antiprompts = new List<string>(inferenceParams.AntiPrompts),
                // Set the token budget.
                RemainedTokens = inferenceParams.MaxTokens,
                ReturnValue = false,
                WaitForInput = false,
                // Determine whether to save session (if a session file exists and the session tokens
                // do not completely match the input tokens).
                NeedToSaveSession = !string.IsNullOrEmpty(_pathSession) && _n_matching_session_tokens < _embed_inps.Count
            };

            // Preprocess the input prompt (or continuation).
            await PreprocessInputs(text, args);

            // Main generation loop.
            while (await GetLoopCondition(args))
            {
                // Respect cancellation requests.
                if (cancellationToken.IsCancellationRequested)
                {
                    break;
                }
                // Perform inference (e.g., decoding, sampling) and update internal state.
                await InferInternal(inferenceParams, args);

                // If new tokens were generated (ReturnValue flag is set)...
                if (args.ReturnValue)
                {
                    // Add the new tokens to the streaming decoder.
                    _decoder.AddRange(_embeds);
                    // Yield the current decoded string.
                    yield return _decoder.Read();
                }

                // Post-process to decide if generation should break or if extra outputs should be emitted.
                var (breakGeneration, extraOutputs) = await PostProcess(inferenceParams, args);
                if (extraOutputs is { Count: > 0 })
                {
                    foreach (var item in extraOutputs)
                    {
                        yield return item;
                    }
                }
                // If instructed to break generation, exit the loop.
                if (breakGeneration)
                {
                    break;
                }
            }
        }

        /// <summary>
        /// Asynchronously pre-fills the prompt through the model to compute the KV cache,
        /// without generating any new tokens. This can reduce latency when the user eventually inputs text.
        /// </summary>
        /// <param name="prompt">The prompt to prefill.</param>
        /// <returns>A task representing the asynchronous prefill operation.</returns>
        public virtual async Task PrefillPromptAsync(string prompt)
        {
            // Create inference parameters with zero tokens to generate.
            var inferenceParams = new InferenceParams
            {
                MaxTokens = 0
            };
            // Set up state arguments for pre-filling; no tokens will be generated and we wait for input.
            var args = new InferStateArgs
            {
                Antiprompts = new List<string>(),
                RemainedTokens = 0,
                ReturnValue = false,
                WaitForInput = true,
                NeedToSaveSession = false
            };

            // Preprocess the prompt.
            await PreprocessInputs(prompt, args);
            // First inference pass: adds the prompt to the _embeds.
            await InferInternal(inferenceParams, args);
            // Second inference pass: processes the prompt through decode (updates KV cache).
            await InferInternal(inferenceParams, args);
        }   

        /// <summary>
        /// Internal class holding state arguments used during a single inference run.
        /// </summary>
        protected class InferStateArgs
        {
            /// <summary>
            /// A list of antiprompt strings. These are used to signal when to stop or wait for input.
            /// </summary>
            public IList<string>? Antiprompts { get; set; }
            /// <summary>
            /// The remaining number of tokens allowed to be generated (n_remain).
            /// </summary>
            public int RemainedTokens { get; set; }
            /// <summary>
            /// Flag indicating if a new token was generated and should be returned.
            /// </summary>
            public bool ReturnValue { get; set; }
            /// <summary>
            /// Flag indicating if the executor should wait for additional input.
            /// </summary>
            public bool WaitForInput { get; set; }
            /// <summary>
            /// Flag indicating whether the session should be saved after inference.
            /// </summary>
            public bool NeedToSaveSession { get; set; }
        }

        /// <summary>
        /// Base state data for an executor.
        /// This class is used for saving and restoring the state of an executor.
        /// </summary>
        [JsonConverter(typeof(PolymorphicJSONConverter<ExecutorBaseState>))]
        public class ExecutorBaseState
        {
            // The count of tokens processed before the current inference.
            [JsonPropertyName("n_past")]
            public int PastTokensCount { get; set; }

            // The count of tokens consumed during the current inference.
            [JsonPropertyName("n_consumed")]
            public int ConsumedTokensCount { get; set; }

            // The count of session tokens that have been consumed.
            [JsonPropertyName("n_session_consumed")]
            public int ConsumedSessionCount { get; set; }

            // The number of tokens in the session that match the prompt.
            [JsonPropertyName("n_matching_session_tokens")]
            public int MatchingSessionTokensCount { get; set; }

            // The file path of the session file.
            [JsonPropertyName("path_session")]
            public string? SessionFilePath { get; set; }

            // The tokens that were generated/processed.
            [JsonPropertyName("embd")]
            public LLamaToken[] Embeds { get; set; }

            // The tokens obtained from the input prompt.
            [JsonPropertyName("embd_inps")]
            public LLamaToken[] EmbedInps { get; set; }

            // The tokens loaded from a session file.
            [JsonPropertyName("session_tokens")]
            public LLamaToken[] SessionTokens { get; set; }

            // The last tokens generated by the model.
            [JsonPropertyName("last_n_tokens")]
            public LLamaToken[] LastTokens { get; set; }

            // The maximum capacity of the last tokens queue.
            [JsonPropertyName("last_tokens_maximum_count")]
            public int LastTokensCapacity { get; set; }

            // Optionally, the Mirostat mu parameter (if using Mirostat sampling).
            [JsonPropertyName("mirostat_mu")]
            public float? MirostatMu { get; set; }
        }
    }
}
